import pandas as pd
import numpy as np
from combat.pycombat import pycombat
from sklearn.decomposition import PCA
from sklearn.cluster import AgglomerativeClustering
from sklearn.metrics import silhouette_score, adjusted_rand_score, normalized_mutual_info_score
from sklearn.metrics import calinski_harabasz_score, davies_bouldin_score
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.patches import Ellipse
from scipy.stats import chi2
from scipy.cluster.hierarchy import dendrogram, linkage, fcluster
from scipy.spatial.distance import pdist, squareform
import warnings
from sklearn.preprocessing import StandardScaler
warnings.filterwarnings('ignore')

# è®¾ç½®matplotlibçš„å­—ä½“å’Œæ ·å¼
plt.rcParams['font.size'] = 12
plt.rcParams['font.family'] = 'Arial'
plt.rcParams['axes.linewidth'] = 1.5
plt.rcParams['axes.spines.top'] = False
plt.rcParams['axes.spines.right'] = False
plt.rcParams['xtick.major.size'] = 6
plt.rcParams['ytick.major.size'] = 6
plt.rcParams['xtick.major.width'] = 1.5
plt.rcParams['ytick.major.width'] = 1.5

def confidence_ellipse(x, y, ax, n_std=1.0, facecolor='none', edgecolor='red', linewidth=2, alpha=0.5):
    """
    ç»˜åˆ¶ç½®ä¿¡æ¤­åœ†
    
    Parameters:
    -----------
    x, y : array-like, shape (n, )
        æ•°æ®ç‚¹çš„x, yåæ ‡
    ax : matplotlib.axes.Axes
        ç»˜å›¾çš„axeså¯¹è±¡
    n_std : float
        æ ‡å‡†å·®çš„å€æ•°ï¼Œç”¨äºç¡®å®šæ¤­åœ†çš„å¤§å°ï¼ˆé»˜è®¤1.0ï¼ŒåŸæ¥æ˜¯2.0ï¼‰
    """
    if x.size != y.size:
        raise ValueError("x and y must be the same size")
    
    # è®¡ç®—åæ–¹å·®çŸ©é˜µ
    cov = np.cov(x, y)
    
    # è®¡ç®—æ¤­åœ†çš„å‚æ•°
    eigenvals, eigenvecs = np.linalg.eigh(cov)
    order = eigenvals.argsort()[::-1]
    eigenvals, eigenvecs = eigenvals[order], eigenvecs[:, order]
    
    # è®¡ç®—æ¤­åœ†çš„è§’åº¦
    angle = np.degrees(np.arctan2(*eigenvecs[:, 0][::-1]))
    
    # è®¡ç®—æ¤­åœ†çš„å®½åº¦å’Œé«˜åº¦
    width, height = 2 * n_std * np.sqrt(eigenvals)
    
    # ç»˜åˆ¶æ¤­åœ†
    ellip = Ellipse(xy=(np.mean(x), np.mean(y)), width=width, height=height, angle=angle,
                    facecolor=facecolor, edgecolor=edgecolor, linewidth=linewidth, alpha=alpha)
    
    return ax.add_patch(ellip)

def plot_pca(data, batch_info, title, ax):
    """
    ç»˜åˆ¶å‘è¡¨çº§åˆ«çš„PCAæ•£ç‚¹å›¾
    
    Parameters:
    -----------
    data: è¡¨è¾¾çŸ©é˜µ (åŸºå›  x æ ·æœ¬)
    batch_info: åŒ…å«æ‰¹æ¬¡ä¿¡æ¯çš„Series
    title: å›¾è¡¨æ ‡é¢˜
    ax: matplotlibçš„Axeså¯¹è±¡
    """
    # Sklearnçš„PCAæœŸæœ›æ ·æœ¬ä½œä¸ºè¡Œï¼Œå› æ­¤éœ€è¦è½¬ç½®æ•°æ®
    pca = PCA(n_components=2)
    principal_components = pca.fit_transform(data.T)

    # åˆ›å»ºä¸€ä¸ªç”¨äºç»˜å›¾çš„DataFrame
    pca_df = pd.DataFrame(data=principal_components,
                          columns=['Dim1', 'Dim2'],
                          index=data.columns)
    pca_df['Group'] = batch_info.values

    # è·å–æ¯ä¸ªä¸»æˆåˆ†è§£é‡Šçš„æ–¹å·®ç™¾åˆ†æ¯”
    explained_variance = pca.explained_variance_ratio_ * 100

    # NatureæœŸåˆŠå¸¸ç”¨é…è‰²
    nature_colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f']
    
    # ä¸“ç”¨äºMYC/PVT1åˆ†ç»„çš„é…è‰²
    myc_pvt1_colors = {
        'hi_hi': '#d62728',  # çº¢è‰² - é«˜è¡¨è¾¾
        'hi_lo': '#ff7f0e',  # æ©™è‰²
        'lo_hi': '#2ca02c',  # ç»¿è‰²
        'lo_lo': '#1f77b4'   # è“è‰² - ä½è¡¨è¾¾
    }
    
    unique_groups = pca_df['Group'].unique()
    
    # åˆ¤æ–­æ˜¯å¦ä¸ºMYC/PVT1åˆ†ç»„
    is_myc_pvt1 = set(unique_groups).issubset({'hi_hi', 'hi_lo', 'lo_hi', 'lo_lo'})
    
    # é€‰æ‹©é…è‰²æ–¹æ¡ˆ
    if is_myc_pvt1:
        # å¦‚æœæ˜¯MYC/PVT1åˆ†ç»„ï¼Œä½¿ç”¨ä¸“ç”¨é…è‰²
        colors = [myc_pvt1_colors.get(group, nature_colors[i]) for i, group in enumerate(unique_groups)]
    else:
        colors = [nature_colors[i % len(nature_colors)] for i in range(len(unique_groups))]

    # æ‰¹æ¬¡åç§°æ˜ å°„
    batch_name_mapping = {
        'IMvigor210': 'Mariathasan et al.',
        'liu': 'Liu et al.',
        'ra': 'Riaz et al.',
        'hugo': 'Hugo et al.'
    }
    
    # ç»˜åˆ¶æ•£ç‚¹å›¾
    for i, group in enumerate(unique_groups):
        group_data = pca_df[pca_df['Group'] == group]
        # ä½¿ç”¨æ˜ å°„åçš„æ‰¹æ¬¡åç§°
        display_name = batch_name_mapping.get(group, group)
        ax.scatter(group_data['Dim1'], group_data['Dim2'],
                  c=colors[i],
                  s=60,
                  alpha=0.8,
                  label=display_name,
                  edgecolors='white',
                  linewidth=0.5)
        
        # åªä¸ºcohortåˆ†ç»„æ·»åŠ ç½®ä¿¡æ¤­åœ†
        if not is_myc_pvt1 and len(group_data) >= 3:
            try:
                confidence_ellipse(
                    group_data['Dim1'], group_data['Dim2'],
                    ax, n_std=2.0,  # 2ä¸ªæ ‡å‡†å·®
                    edgecolor=colors[i],
                    facecolor=colors[i],
                    alpha=0.15,
                    linewidth=2
                )
            except Exception as e:
                print(f"è·³è¿‡ç»„ {group} çš„æ¤­åœ†ç»˜åˆ¶: {str(e)}")

    # è®¾ç½®å­—ä½“å’Œæ ·å¼
    plt.rcParams['font.family'] = 'Arial'
    plt.rcParams['pdf.fonttype'] = 42
    
    # è®¾ç½®æ ‡é¢˜å’Œè½´æ ‡ç­¾
    ax.set_title(title, fontsize=16, fontweight='bold', pad=20)
    ax.set_xlabel(f'PC1 ({explained_variance[0]:.1f}%)', fontsize=14, fontweight='bold')
    ax.set_ylabel(f'PC2 ({explained_variance[1]:.1f}%)', fontsize=14, fontweight='bold')
    
    # æ˜¾ç¤ºå®Œæ•´çš„çŸ©å½¢è¾¹æ¡†
    for spine in ax.spines.values():
        spine.set_visible(True)
        spine.set_linewidth(1.5)
    
    # è®¾ç½®åˆ»åº¦æ ‡ç­¾å­—ä½“å¤§å°
    ax.tick_params(axis='both', labelsize=12)
    
    # æ·»åŠ ç½‘æ ¼çº¿
    ax.grid(True, alpha=0.3)
    
    # ä¼˜åŒ–å›¾ä¾‹
    legend = ax.legend(title='Group',
                      frameon=True,
                      fancybox=True,
                      framealpha=1,
                      shadow=True,
                      bbox_to_anchor=(1.05, 1),
                      loc='upper left',
                      fontsize=16)  # åŸæ¥æ˜¯12ï¼Œè°ƒæ•´ä¸º1.3å€
    legend.get_title().set_fontsize(17)  # åŸæ¥æ˜¯13ï¼Œè°ƒæ•´ä¸º1.3å€
    legend.get_title().set_fontweight('bold')
    
    # è®¾ç½®èƒŒæ™¯ä¸ºç™½è‰²
    ax.set_facecolor('white')
    
    # è°ƒæ•´å¸ƒå±€ä»¥é€‚åº”å›¾ä¾‹
    plt.tight_layout()

def plot_tsne(data, batch_info, title, ax, perplexity=30, random_state=42):
    """
    ä½¿ç”¨t-SNEè¿›è¡Œå‘è¡¨çº§åˆ«çš„é™ç»´å¯è§†åŒ–
    
    Parameters:
    -----------
    data: è¡¨è¾¾çŸ©é˜µ (åŸºå›  x æ ·æœ¬)
    batch_info: åŒ…å«æ‰¹æ¬¡ä¿¡æ¯çš„Series
    title: å›¾è¡¨æ ‡é¢˜
    ax: matplotlibçš„Axeså¯¹è±¡
    """
    print(f"    æ­£åœ¨è¿è¡Œt-SNEé™ç»´: {title}")
    
    # t-SNEé™ç»´
    tsne = TSNE(n_components=2, perplexity=perplexity, random_state=random_state, 
                n_iter=1000, verbose=0)
    embedding = tsne.fit_transform(data.T)
    
    # åˆ›å»ºç”¨äºç»˜å›¾çš„DataFrame
    tsne_df = pd.DataFrame(data=embedding, columns=['tSNE1', 'tSNE2'], index=data.columns)
    tsne_df['Group'] = batch_info.values
    
    # NatureæœŸåˆŠå¸¸ç”¨é…è‰²
    nature_colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f']
    
    # ä¸“ç”¨äºMYC/PVT1åˆ†ç»„çš„é…è‰²
    myc_pvt1_colors = {
        'hi_hi': '#d62728',  # çº¢è‰² - é«˜è¡¨è¾¾
        'hi_lo': '#ff7f0e',  # æ©™è‰²
        'lo_hi': '#2ca02c',  # ç»¿è‰²
        'lo_lo': '#1f77b4'   # è“è‰² - ä½è¡¨è¾¾
    }
    
    unique_groups = tsne_df['Group'].unique()
    
    # åˆ¤æ–­æ˜¯å¦ä¸ºMYC/PVT1åˆ†ç»„
    is_myc_pvt1 = set(unique_groups).issubset({'hi_hi', 'hi_lo', 'lo_hi', 'lo_lo'})
    
    # é€‰æ‹©é…è‰²æ–¹æ¡ˆ
    if is_myc_pvt1:
        # å¦‚æœæ˜¯MYC/PVT1åˆ†ç»„ï¼Œä½¿ç”¨ä¸“ç”¨é…è‰²
        colors = [myc_pvt1_colors.get(group, nature_colors[i]) for i, group in enumerate(unique_groups)]
    else:
        colors = [nature_colors[i % len(nature_colors)] for i in range(len(unique_groups))]

    # æ‰¹æ¬¡åç§°æ˜ å°„
    batch_name_mapping = {
        'IMvigor210': 'Mariathasan et al.',
        'liu': 'Liu et al.',
        'ra': 'Riaz et al.',
        'hugo': 'Hugo et al.'
    }
    
    # ç»˜åˆ¶æ•£ç‚¹å›¾
    for i, group in enumerate(unique_groups):
        group_data = tsne_df[tsne_df['Group'] == group]
        # ä½¿ç”¨æ˜ å°„åçš„æ‰¹æ¬¡åç§°
        display_name = batch_name_mapping.get(group, group)
        ax.scatter(group_data['tSNE1'], group_data['tSNE2'],
                  c=colors[i],
                  s=60,
                  alpha=0.8,
                  label=display_name,
                  edgecolors='white',
                  linewidth=0.5)
        
        # åªä¸ºcohortåˆ†ç»„æ·»åŠ ç½®ä¿¡æ¤­åœ†
        if not is_myc_pvt1 and len(group_data) >= 3:
            try:
                confidence_ellipse(
                    group_data['tSNE1'], group_data['tSNE2'],
                    ax, n_std=2.0,  # 2ä¸ªæ ‡å‡†å·®
                    edgecolor=colors[i],
                    facecolor=colors[i],
                    alpha=0.15,
                    linewidth=2
                )
            except Exception as e:
                print(f"è·³è¿‡ç»„ {group} çš„æ¤­åœ†ç»˜åˆ¶: {str(e)}")

    # è®¾ç½®å­—ä½“å’Œæ ·å¼
    plt.rcParams['font.family'] = 'Arial'
    plt.rcParams['pdf.fonttype'] = 42
    
    # è®¾ç½®æ ‡é¢˜å’Œè½´æ ‡ç­¾
    ax.set_title(title, fontsize=16, fontweight='bold', pad=20)
    ax.set_xlabel('t-SNE1', fontsize=14, fontweight='bold')
    ax.set_ylabel('t-SNE2', fontsize=14, fontweight='bold')
    
    # æ˜¾ç¤ºå®Œæ•´çš„çŸ©å½¢è¾¹æ¡†
    for spine in ax.spines.values():
        spine.set_visible(True)
        spine.set_linewidth(1.5)
    
    # è®¾ç½®åˆ»åº¦æ ‡ç­¾å­—ä½“å¤§å°
    ax.tick_params(axis='both', labelsize=12)
    
    # æ·»åŠ ç½‘æ ¼çº¿
    ax.grid(True, alpha=0.3)
    
    # ä¼˜åŒ–å›¾ä¾‹
    legend = ax.legend(title='Group',
                      frameon=True,
                      fancybox=True,
                      framealpha=1,
                      shadow=True,
                      bbox_to_anchor=(1.05, 1),
                      loc='upper left',
                      fontsize=16)  # åŸæ¥æ˜¯12ï¼Œè°ƒæ•´ä¸º1.3å€
    legend.get_title().set_fontsize(17)  # åŸæ¥æ˜¯13ï¼Œè°ƒæ•´ä¸º1.3å€
    legend.get_title().set_fontweight('bold')
    
    # è®¾ç½®èƒŒæ™¯ä¸ºç™½è‰²
    ax.set_facecolor('white')
    
    # è°ƒæ•´å¸ƒå±€ä»¥é€‚åº”å›¾ä¾‹
    plt.tight_layout()

def evaluate_clustering_metrics(data, batch_labels, title_prefix=""):
    """
    è¯„ä¼°èšç±»è´¨é‡çš„å¤šç§æŒ‡æ ‡
    
    Parameters:
    -----------
    data : pandas.DataFrame
        è¡¨è¾¾çŸ©é˜µ (åŸºå›  x æ ·æœ¬)ï¼Œå°†è¢«è½¬ç½®ç”¨äºèšç±»
    batch_labels : pandas.Series
        æ‰¹æ¬¡æ ‡ç­¾
    title_prefix : str
        æ ‡é¢˜å‰ç¼€
    
    Returns:
    --------
    dict : åŒ…å«å„ç§èšç±»è¯„ä¼°æŒ‡æ ‡çš„å­—å…¸
    """
    
    # è½¬ç½®æ•°æ®ï¼ˆæ ·æœ¬ x åŸºå› ï¼‰ç”¨äºèšç±»
    X = data.T
    
    # è·å–æ‰¹æ¬¡æ ‡ç­¾çš„æ•°å€¼ç¼–ç 
    batch_labels_numeric = pd.Categorical(batch_labels).codes
    
    # è®¡ç®—æ ·æœ¬é—´çš„è·ç¦»çŸ©é˜µ
    print(f"  {title_prefix}è®¡ç®—æ ·æœ¬é—´è·ç¦»çŸ©é˜µ...")
    
    # ä½¿ç”¨ç›¸å…³ç³»æ•°è·ç¦»ï¼ˆ1-ç›¸å…³ç³»æ•°ï¼‰
    # è®¡ç®—æ ·æœ¬é—´ç›¸å…³æ€§
    correlation_matrix = X.T.corr()  # æ ·æœ¬é—´ç›¸å…³æ€§
    distance_matrix = 1 - correlation_matrix
    
    # ç¡®ä¿è·ç¦»çŸ©é˜µéè´Ÿä¸”å¯¹ç§°
    distance_matrix = np.abs(distance_matrix)
    distance_matrix = (distance_matrix + distance_matrix.T) / 2
    
    # è½¬æ¢ä¸ºnumpyæ•°ç»„
    distance_matrix = distance_matrix.values
    np.fill_diagonal(distance_matrix, 0)
    
    # å±‚æ¬¡èšç±»
    print(f"  {title_prefix}æ‰§è¡Œå±‚æ¬¡èšç±»...")
    linkage_matrix = linkage(squareform(distance_matrix), method='ward')
    
    # ç¡®å®šèšç±»æ•°é‡ï¼ˆä½¿ç”¨æ‰¹æ¬¡æ•°é‡ï¼‰
    n_clusters = len(batch_labels.unique())
    cluster_labels = fcluster(linkage_matrix, n_clusters, criterion='maxclust')
    
    # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
    metrics = {}
    
    try:
        # ç¡…è½®å»“ç³»æ•° (èŒƒå›´: -1 åˆ° 1, è¶Šé«˜è¶Šå¥½)
        silhouette_avg = silhouette_score(distance_matrix, cluster_labels, metric='precomputed')
        metrics['silhouette_score'] = silhouette_avg
        
        # è°ƒæ•´å…°å¾·æŒ‡æ•° (èŒƒå›´: -1 åˆ° 1, è¶Šé«˜è¶Šå¥½)
        ari_score = adjusted_rand_score(batch_labels_numeric, cluster_labels)
        metrics['adjusted_rand_index'] = ari_score
        
        # æ ‡å‡†åŒ–äº’ä¿¡æ¯ (èŒƒå›´: 0 åˆ° 1, è¶Šé«˜è¶Šå¥½)
        nmi_score = normalized_mutual_info_score(batch_labels_numeric, cluster_labels)
        metrics['normalized_mutual_info'] = nmi_score
        
        # å¡æ—æ–¯åŸº-å“ˆæ‹‰å·´å…¹æŒ‡æ•° (è¶Šé«˜è¶Šå¥½)
        # æ³¨æ„ï¼šè¿™ä¸ªæŒ‡æ ‡éœ€è¦åŸå§‹ç‰¹å¾ç©ºé—´ï¼Œä¸èƒ½ä½¿ç”¨è·ç¦»çŸ©é˜µ
        ch_score = calinski_harabasz_score(X, cluster_labels)
        metrics['calinski_harabasz_score'] = ch_score
        
        # æˆ´ç»´æ–¯-é²å°”ä¸æŒ‡æ•° (è¶Šä½è¶Šå¥½)
        db_score = davies_bouldin_score(X, cluster_labels)
        metrics['davies_bouldin_score'] = db_score
        
        # æ‰¹æ¬¡æ··åˆåº¦è¯„ä¼°
        batch_purity = calculate_batch_purity(cluster_labels, batch_labels_numeric)
        metrics['batch_purity'] = batch_purity
        
        # æ‰¹æ¬¡åˆ†ç¦»åº¦è¯„ä¼°
        batch_separation = calculate_batch_separation(cluster_labels, batch_labels_numeric)
        metrics['batch_separation'] = batch_separation
        
    except Exception as e:
        print(f"    è­¦å‘Šï¼šè®¡ç®—æŸäº›æŒ‡æ ‡æ—¶å‡ºé”™: {str(e)}")
    
    return metrics, linkage_matrix, cluster_labels

def calculate_batch_purity(cluster_labels, batch_labels):
    """
    è®¡ç®—æ‰¹æ¬¡çº¯åº¦ï¼šè¡¡é‡æ¯ä¸ªèšç±»ä¸­æ˜¯å¦ä¸»è¦ç”±ä¸€ä¸ªæ‰¹æ¬¡ç»„æˆ
    è¿”å›å€¼è¶Šæ¥è¿‘1è¡¨ç¤ºæ‰¹æ¬¡æ•ˆåº”è¶Šå¼ºï¼ˆæ ¡æ­£æ•ˆæœå·®ï¼‰
    """
    unique_clusters = np.unique(cluster_labels)
    total_samples = len(cluster_labels)
    weighted_purity = 0
    
    for cluster_id in unique_clusters:
        cluster_mask = cluster_labels == cluster_id
        cluster_batches = batch_labels[cluster_mask]
        
        if len(cluster_batches) > 0:
            # è®¡ç®—è¯¥èšç±»ä¸­æœ€ä¸»è¦æ‰¹æ¬¡çš„æ¯”ä¾‹
            batch_counts = np.bincount(cluster_batches)
            max_batch_count = np.max(batch_counts)
            cluster_purity = max_batch_count / len(cluster_batches)
            
            # æŒ‰èšç±»å¤§å°åŠ æƒ
            cluster_weight = len(cluster_batches) / total_samples
            weighted_purity += cluster_purity * cluster_weight
    
    return weighted_purity

def calculate_batch_separation(cluster_labels, batch_labels):
    """
    è®¡ç®—æ‰¹æ¬¡åˆ†ç¦»åº¦ï¼šè¡¡é‡ä¸åŒæ‰¹æ¬¡æ˜¯å¦è¢«åˆ†é…åˆ°ä¸åŒèšç±»
    è¿”å›å€¼è¶Šæ¥è¿‘1è¡¨ç¤ºæ‰¹æ¬¡æ•ˆåº”è¶Šå¼ºï¼ˆæ ¡æ­£æ•ˆæœå·®ï¼‰
    """
    unique_batches = np.unique(batch_labels)
    total_separation = 0
    
    for batch_id in unique_batches:
        batch_mask = batch_labels == batch_id
        batch_clusters = cluster_labels[batch_mask]
        
        if len(batch_clusters) > 0:
            # è®¡ç®—è¯¥æ‰¹æ¬¡æ ·æœ¬çš„èšç±»å¤šæ ·æ€§
            cluster_counts = np.bincount(batch_clusters)
            cluster_counts = cluster_counts[cluster_counts > 0]
            
            if len(cluster_counts) > 1:
                # ä½¿ç”¨ç†µæ¥è¡¡é‡èšç±»å¤šæ ·æ€§
                probabilities = cluster_counts / np.sum(cluster_counts)
                # é¿å…log(0)çš„æƒ…å†µ
                probabilities = probabilities[probabilities > 0]
                entropy = -np.sum(probabilities * np.log2(probabilities))
                max_entropy = np.log2(len(cluster_counts))
                separation = entropy / max_entropy if max_entropy > 0 else 0
            else:
                separation = 0  # æ‰€æœ‰æ ·æœ¬éƒ½åœ¨åŒä¸€ä¸ªèšç±»ä¸­
            
            total_separation += separation
    
    return total_separation / len(unique_batches)

def plot_clustering_heatmap(data, batch_labels, linkage_matrix, cluster_labels, title, figsize=(12, 8)):
    """
    ç»˜åˆ¶èšç±»çƒ­å›¾
    """
    try:
        # å‡†å¤‡æ•°æ®
        X = data.T  # æ ·æœ¬ x åŸºå› 
        
        # é€‰æ‹©å‰1000ä¸ªæ–¹å·®æœ€å¤§çš„åŸºå› è¿›è¡Œå¯è§†åŒ–
        gene_vars = data.var(axis=1)
        n_genes = min(1000, len(gene_vars))
        top_genes = gene_vars.nlargest(n_genes).index
        data_subset = data.loc[top_genes]
        
        # åˆ›å»ºæ³¨é‡Šé¢œè‰²
        batch_colors = {'IMvigor210': '#1f77b4', 'Liu et al., Nat Medicine 2019': '#ff7f0e', 'Riaz et al., Cell 2017': '#2ca02c', 'Hugo et al., Cell 2016': '#d62728'}
        
        # åˆ›å»ºé¢œè‰²æ˜ å°„
        batch_color_map = {batch: color for batch, color in batch_colors.items() if batch in batch_labels.unique()}
        
        # ç»˜åˆ¶çƒ­å›¾
        g = sns.clustermap(data_subset, 
                          col_linkage=linkage_matrix,
                          col_cluster=True,
                          row_cluster=True,
                          col_colors=[batch_color_map.get(batch, '#gray') for batch in batch_labels],
                          cmap='RdBu_r',
                          center=0,
                          figsize=figsize,
                          dendrogram_ratio=0.15,
                          colors_ratio=0.02)
        
        g.fig.suptitle(title, fontsize=14, fontweight='bold')
        
        return g.fig
        
    except Exception as e:
        print(f"ç»˜åˆ¶èšç±»çƒ­å›¾æ—¶å‡ºé”™: {str(e)}")
        # è¿”å›ä¸€ä¸ªç®€å•çš„å›¾å½¢
        fig, ax = plt.subplots(figsize=figsize)
        ax.text(0.5, 0.5, f'èšç±»çƒ­å›¾ç”Ÿæˆå¤±è´¥\n{str(e)}', 
                ha='center', va='center', transform=ax.transAxes)
        ax.set_title(title)
        return fig

def plot_clustering_evaluation(metrics_before, metrics_after, figsize=(14, 10)):
    """
    ç»˜åˆ¶èšç±»è¯„ä¼°æŒ‡æ ‡å¯¹æ¯”å›¾
    """
    fig, axes = plt.subplots(2, 3, figsize=figsize)
    axes = axes.flatten()
    
    # å®šä¹‰æŒ‡æ ‡å’Œå…¶ç†æƒ³æ–¹å‘
    metrics_info = {
        'silhouette_score': ('Silhouette Score', 'higher_better'),
        'adjusted_rand_index': ('Adjusted Rand Index', 'higher_better'),
        'normalized_mutual_info': ('Normalized Mutual Info', 'higher_better'),
        'calinski_harabasz_score': ('Calinski-Harabasz Score', 'higher_better'),
        'davies_bouldin_score': ('Davies-Bouldin Score', 'lower_better'),
        'batch_purity': ('Batch Purity', 'lower_better')
    }
    
    for i, (metric_key, (metric_name, direction)) in enumerate(metrics_info.items()):
        if i >= len(axes):
            break
            
        ax = axes[i]
        
        # è·å–æ•°å€¼
        before_val = metrics_before.get(metric_key, 0)
        after_val = metrics_after.get(metric_key, 0)
        
        # ç»˜åˆ¶æ¡å½¢å›¾
        categories = ['Before ComBat', 'After ComBat']
        values = [before_val, after_val]
        
        bars = ax.bar(categories, values, color=['#ff7f7f', '#7f7fff'], alpha=0.8)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, val in zip(bars, values):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height + height*0.01,
                   f'{val:.3f}', ha='center', va='bottom', fontweight='bold')
        
        # è®¾ç½®æ ‡é¢˜å’Œæ ‡ç­¾
        ax.set_title(metric_name, fontsize=12, fontweight='bold')
        ax.set_ylabel('Score', fontsize=10)
        
        # æ ¹æ®æŒ‡æ ‡æ–¹å‘è®¾ç½®é¢œè‰²
        if direction == 'higher_better':
            if after_val > before_val:
                ax.set_facecolor('#e8f5e8')  # æµ…ç»¿è‰²èƒŒæ™¯è¡¨ç¤ºæ”¹å–„
            else:
                ax.set_facecolor('#ffe8e8')  # æµ…çº¢è‰²èƒŒæ™¯è¡¨ç¤ºæ¶åŒ–
        else:  # lower_better
            if after_val < before_val:
                ax.set_facecolor('#e8f5e8')  # æµ…ç»¿è‰²èƒŒæ™¯è¡¨ç¤ºæ”¹å–„
            else:
                ax.set_facecolor('#ffe8e8')  # æµ…çº¢è‰²èƒŒæ™¯è¡¨ç¤ºæ¶åŒ–
        
        # ç¾åŒ–
        ax.grid(True, alpha=0.3)
        ax.set_axisbelow(True)
        
        # è®¾ç½®yè½´èŒƒå›´
        if metric_key in ['silhouette_score', 'adjusted_rand_index']:
            ax.set_ylim(-1, 1)
        elif metric_key in ['normalized_mutual_info', 'batch_purity']:
            ax.set_ylim(0, 1)
    
    plt.tight_layout()
    return fig


# ==============================================================================
# 1. åŠ è½½å››ä¸ªè¿‡æ»¤åçš„è¡¨è¾¾çŸ©é˜µæ–‡ä»¶
# ==============================================================================
# å®šä¹‰æ–‡ä»¶è·¯å¾„
file_paths = {
    'IMvigor210': 'IMvigor210_filtered1.txt',
    'liu': 'liu_filtered1.txt', 
    'ra': 'ra_filtered1.txt',
    'hugo': 'hugo_filtered1.txt'
}

print("æ­£åœ¨åŠ è½½è¿‡æ»¤åçš„è¡¨è¾¾çŸ©é˜µæ–‡ä»¶...")
datasets = {}
all_samples = []
all_batches = []
sample_mapping = {}  # å­˜å‚¨åŸå§‹æ ·æœ¬ååˆ°å‰ç¼€æ ·æœ¬åçš„æ˜ å°„

# åŠ è½½æ¯ä¸ªæ–‡ä»¶
for batch_name, file_path in file_paths.items():
    try:
        # è¯»å–è¡¨è¾¾çŸ©é˜µï¼ˆåŸºå› ä¸ºè¡Œï¼Œæ ·æœ¬ä¸ºåˆ—ï¼‰
        df = pd.read_csv(file_path, sep='\t', index_col=0)
        
        # ä¸ºæ ·æœ¬åæ·»åŠ å‰ç¼€ä»¥åŒºåˆ†ä¸åŒæ•°æ®é›†
        original_samples = df.columns.tolist()
        prefixed_samples = [f"{batch_name}_{sample}" for sample in original_samples]
        df.columns = prefixed_samples
        
        # è®°å½•åŸå§‹æ ·æœ¬ååˆ°å‰ç¼€æ ·æœ¬åçš„æ˜ å°„
        for orig, prefixed in zip(original_samples, prefixed_samples):
            sample_mapping[orig] = prefixed
        
        datasets[batch_name] = df
        
        # æ”¶é›†æ ·æœ¬ä¿¡æ¯
        all_samples.extend(prefixed_samples)
        all_batches.extend([batch_name] * len(prefixed_samples))
        
        print(f"  {batch_name}: {df.shape[0]}ä¸ªåŸºå› , {df.shape[1]}ä¸ªæ ·æœ¬")
        print(f"    æ ·æœ¬åç¤ºä¾‹: {prefixed_samples[0]} -> {prefixed_samples[-1]}")
        
    except FileNotFoundError:
        print(f"  è­¦å‘Šï¼šæ–‡ä»¶ {file_path} ä¸å­˜åœ¨ï¼Œè·³è¿‡...")
    except Exception as e:
        print(f"  é”™è¯¯ï¼šåŠ è½½ {file_path} å¤±è´¥: {str(e)}")

# æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„æ•°æ®é›†
if not datasets:
    print("é”™è¯¯ï¼šæ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„æ•°æ®æ–‡ä»¶ï¼")
    exit(1)

print(f"\næˆåŠŸåŠ è½½ {len(datasets)} ä¸ªæ•°æ®é›†")

# æ‰¾åˆ°æ‰€æœ‰æ•°æ®é›†çš„å…±åŒåŸºå› 
print("æ­£åœ¨å¯»æ‰¾å…±åŒåŸºå› ...")
common_genes = None
for batch_name, df in datasets.items():
    if common_genes is None:
        common_genes = set(df.index)
    else:
        common_genes = common_genes.intersection(set(df.index))

common_genes = sorted(list(common_genes))
print(f"å…±åŒåŸºå› æ•°é‡: {len(common_genes)}")

# è¿‡æ»¤æ¯ä¸ªæ•°æ®é›†ï¼Œåªä¿ç•™å…±åŒåŸºå› 
print("æ­£åœ¨è¿‡æ»¤æ•°æ®é›†...")
filtered_datasets = {}
for batch_name, df in datasets.items():
    filtered_df = df.loc[common_genes]
    filtered_datasets[batch_name] = filtered_df
    print(f"  {batch_name}: {filtered_df.shape[0]}ä¸ªåŸºå› , {filtered_df.shape[1]}ä¸ªæ ·æœ¬")

# åˆå¹¶æ‰€æœ‰è¡¨è¾¾çŸ©é˜µ
print("æ­£åœ¨åˆå¹¶è¡¨è¾¾çŸ©é˜µ...")
expression_data = pd.concat(list(filtered_datasets.values()), axis=1)

# åˆ›å»ºæ‰¹æ¬¡ä¿¡æ¯
batch_info = pd.Series(all_batches, index=all_samples, name="Batch")

print("æ•°æ®åŠ è½½å®Œæˆã€‚")
print("åˆå¹¶åè¡¨è¾¾çŸ©é˜µç»´åº¦ (åŸºå›  x æ ·æœ¬):", expression_data.shape)
print("æ‰¹æ¬¡ä¿¡æ¯é¢„è§ˆ:")
print(batch_info.value_counts())
print(f"æ ·æœ¬åæ·»åŠ å‰ç¼€åæ€»æ•°: {len(all_samples)}")
print(f"æ ·æœ¬åæ˜ å°„å…³ç³»æ€»æ•°: {len(sample_mapping)}")
print("å‰ç¼€æ ·æœ¬åç¤ºä¾‹:")
for i, (orig, prefixed) in enumerate(list(sample_mapping.items())[:3]):
    print(f"  {orig} -> {prefixed}")
if len(sample_mapping) > 3:
    print(f"  ... ä»¥åŠå…¶ä»–{len(sample_mapping)-3}ä¸ªæ ·æœ¬")

# æ˜¾ç¤ºè¡¨è¾¾é‡åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
print("\nè¡¨è¾¾é‡åŸºæœ¬ç»Ÿè®¡:")
print(f"  æœ€å°å€¼: {expression_data.min().min():.4f}")
print(f"  æœ€å¤§å€¼: {expression_data.max().max():.4f}")
print(f"  å¹³å‡å€¼: {expression_data.mean().mean():.4f}")
print(f"  ä¸­ä½æ•°: {expression_data.median().median():.4f}")
print("-" * 30)

# ç¡®ä¿è¡¨è¾¾çŸ©é˜µçš„åˆ—å (æ ·æœ¬å) å’Œ batch_info çš„ç´¢å¼•å®Œå…¨ä¸€è‡´
assert all(expression_data.columns == batch_info.index), "æ ·æœ¬åä¸ä¸€è‡´ï¼"
print("æ ·æœ¬åä¸€è‡´æ€§æ£€æŸ¥é€šè¿‡ã€‚")

# æ£€æŸ¥MYCå’ŒPVT1åŸºå› æ˜¯å¦å­˜åœ¨
print("\nå…³é”®åŸºå› æ£€æŸ¥:")
if 'MYC' in expression_data.index:
    print("  âœ“ MYCåŸºå› å­˜åœ¨")
    myc_stats = expression_data.loc['MYC'].describe()
    print(f"    MYCè¡¨è¾¾é‡ç»Ÿè®¡: å¹³å‡å€¼={myc_stats['mean']:.4f}, ä¸­ä½æ•°={myc_stats['50%']:.4f}, æ ‡å‡†å·®={myc_stats['std']:.4f}")
else:
    print("  âœ— MYCåŸºå› ä¸å­˜åœ¨")

if 'PVT1' in expression_data.index:
    print("  âœ“ PVT1åŸºå› å­˜åœ¨")
    pvt1_stats = expression_data.loc['PVT1'].describe()
    print(f"    PVT1è¡¨è¾¾é‡ç»Ÿè®¡: å¹³å‡å€¼={pvt1_stats['mean']:.4f}, ä¸­ä½æ•°={pvt1_stats['50%']:.4f}, æ ‡å‡†å·®={pvt1_stats['std']:.4f}")
else:
    print("  âœ— PVT1åŸºå› ä¸å­˜åœ¨")

# åŠ è½½MYC_PVT1æ³¨é‡Šæ–‡ä»¶
print("\nåŠ è½½MYC_PVT1æ³¨é‡Šæ–‡ä»¶...")
try:
    annotation_df = pd.read_csv('MYC_PVT1_annotation.txt', sep='\t')
    print(f"  æ³¨é‡Šæ–‡ä»¶åŠ è½½æˆåŠŸï¼ŒåŒ…å«{len(annotation_df)}ä¸ªæ ·æœ¬")
    
    # æ£€æŸ¥æ¯ä¸ªæ ·æœ¬æ˜¯å¦åœ¨è¡¨è¾¾çŸ©é˜µä¸­
    valid_samples = []
    valid_statuses = []
    
    for idx, row in annotation_df.iterrows():
        sample = row['Sample']
        status = row['MYC_PVT1_Status']
        
        # æ£€æŸ¥è¯¥æ ·æœ¬æ˜¯å¦åœ¨è¡¨è¾¾çŸ©é˜µä¸­
        if sample in expression_data.columns:
            valid_samples.append(sample)
            valid_statuses.append(status)
        else:
            print(f"    è­¦å‘Šï¼šæ ·æœ¬ {sample} ä¸åœ¨è¡¨è¾¾çŸ©é˜µä¸­")
    
    # åˆ›å»ºhi_hi/lo_loåˆ†ç»„ä¿¡æ¯
    myc_pvt1_info = pd.Series(valid_statuses, index=valid_samples, name="MYC_PVT1_Status")
    
    print(f"  æˆåŠŸåŒ¹é…æ ·æœ¬æ•°: {len(valid_samples)}")
    print(f"  åˆ†ç»„ç»Ÿè®¡:")
    print(f"    {myc_pvt1_info.value_counts().to_dict()}")
    
except FileNotFoundError:
    print("  è­¦å‘Šï¼šMYC_PVT1_annotation.txtæ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·å…ˆè¿è¡Œæ•°æ®å¤„ç†è„šæœ¬")
    myc_pvt1_info = None
except Exception as e:
    print(f"  é”™è¯¯ï¼šåŠ è½½æ³¨é‡Šæ–‡ä»¶å¤±è´¥: {str(e)}")
    myc_pvt1_info = None


# ==============================================================================
# 2. è¿è¡ŒComBatè¿›è¡Œæ‰¹æ¬¡æ ¡æ­£
# ==============================================================================
# ComBatå‡½æ•°éœ€è¦è¡¨è¾¾æ•°æ®ï¼ˆåŸºå› xæ ·æœ¬ï¼‰å’Œæ‰¹æ¬¡ä¿¡æ¯
print("æ­£åœ¨ä½¿ç”¨ComBatè¿›è¡Œæ‰¹æ¬¡æ ¡æ­£...")

# æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„æ‰¹æ¬¡è¿›è¡ŒComBatåˆ†æ
unique_batches = batch_info.unique()
if len(unique_batches) < 2:
    print("è­¦å‘Šï¼šåªæœ‰ä¸€ä¸ªæ‰¹æ¬¡ï¼Œæ— æ³•è¿›è¡Œæ‰¹æ¬¡æ ¡æ­£ï¼")
    corrected_data = expression_data.copy()
else:
    print(f"æ£€æµ‹åˆ° {len(unique_batches)} ä¸ªæ‰¹æ¬¡: {list(unique_batches)}")
    
# æ³¨æ„ï¼šComBatå¤„ç†çš„æ˜¯æ•°å€¼å‹æ•°æ®ï¼Œå¦‚æœæ•°æ®ä¸­æœ‰éæ•°å€¼ï¼Œéœ€è¦æå‰å¤„ç†
corrected_data = pycombat(expression_data, batch_info)
print("ComBatæ ¡æ­£å®Œæˆã€‚")

print("æ ¡æ­£åæ•°æ®ç»´åº¦:", corrected_data.shape)
print("-" * 30)

# ==============================================================================
# 3. Cohortèšç±»åˆ†æè¯„ä¼°æ‰¹æ¬¡æ•ˆåº”æ ¡æ­£æ•ˆæœ
# ==============================================================================
print("æ­£åœ¨è¿›è¡ŒCohortèšç±»åˆ†æè¯„ä¼°æ‰¹æ¬¡æ•ˆåº”æ ¡æ­£æ•ˆæœ...")

# è¯„ä¼°æ ¡æ­£å‰çš„èšç±»è´¨é‡
print("\n=== æ ¡æ­£å‰èšç±»è´¨é‡è¯„ä¼° ===")
metrics_before, linkage_before, clusters_before = evaluate_clustering_metrics(
    expression_data, batch_info, "æ ¡æ­£å‰ï¼š"
)

# è¯„ä¼°æ ¡æ­£åçš„èšç±»è´¨é‡
print("\n=== æ ¡æ­£åèšç±»è´¨é‡è¯„ä¼° ===")
metrics_after, linkage_after, clusters_after = evaluate_clustering_metrics(
    corrected_data, batch_info, "æ ¡æ­£åï¼š"
)

# æ‰“å°è¯„ä¼°ç»“æœ
print("\n" + "="*60)
print("COHORTèšç±»åˆ†æç»“æœæ±‡æ€»")
print("="*60)

print(f"\nğŸ“Š èšç±»è´¨é‡æŒ‡æ ‡å¯¹æ¯”:")
print(f"{'æŒ‡æ ‡':<25} {'æ ¡æ­£å‰':<12} {'æ ¡æ­£å':<12} {'å˜åŒ–':<10} {'æ•ˆæœ'}")
print("-" * 70)

for metric_key, metric_name in [
    ('silhouette_score', 'Silhouette Score'),
    ('adjusted_rand_index', 'Adjusted Rand Index'),
    ('normalized_mutual_info', 'Normalized Mutual Info'),
    ('calinski_harabasz_score', 'Calinski-Harabasz'),
    ('davies_bouldin_score', 'Davies-Bouldin'),
    ('batch_purity', 'Batch Purity'),
    ('batch_separation', 'Batch Separation')
]:
    before_val = metrics_before.get(metric_key, 0)
    after_val = metrics_after.get(metric_key, 0)
    change = after_val - before_val
    
    # åˆ¤æ–­æ•ˆæœï¼ˆæ ¹æ®æŒ‡æ ‡ç‰¹æ€§ï¼‰
    if metric_key in ['davies_bouldin_score', 'batch_purity']:
        # è¶Šä½è¶Šå¥½çš„æŒ‡æ ‡
        effect = "âœ“ æ”¹å–„" if change < 0 else "âœ— æ¶åŒ–"
    else:
        # è¶Šé«˜è¶Šå¥½çš„æŒ‡æ ‡
        effect = "âœ“ æ”¹å–„" if change > 0 else "âœ— æ¶åŒ–"
    
    print(f"{metric_name:<25} {before_val:<12.3f} {after_val:<12.3f} {change:<+10.3f} {effect}")

# ç»˜åˆ¶èšç±»è¯„ä¼°æŒ‡æ ‡å¯¹æ¯”å›¾
print(f"\nğŸ“ˆ æ­£åœ¨ç”Ÿæˆèšç±»è¯„ä¼°æŒ‡æ ‡å¯¹æ¯”å›¾...")
fig_metrics = plot_clustering_evaluation(metrics_before, metrics_after)
fig_metrics.suptitle('Cohort Clustering Evaluation: Before vs After ComBat Correction', 
                     fontsize=16, fontweight='bold', y=0.98)
plt.show()

# ä¿å­˜èšç±»è¯„ä¼°å›¾
fig_metrics.savefig('cohort_clustering_evaluation.pdf', dpi=300, bbox_inches='tight', facecolor='white')
print("èšç±»è¯„ä¼°å›¾å·²ä¿å­˜åˆ°: cohort_clustering_evaluation.pdf")

# ä¿å­˜èšç±»ç»“æœ
clustering_results = pd.DataFrame({
    'Sample': expression_data.columns,
    'Batch': batch_info.values,
    'Cluster_Before': clusters_before,
    'Cluster_After': clusters_after
})
clustering_results.to_csv('cohort_clustering_results.txt', sep='\t', index=False)
print("èšç±»ç»“æœå·²ä¿å­˜åˆ°: cohort_clustering_results.txt")

# ==============================================================================
# 4. åˆ›å»ºt-SNEåˆ†æå›¾è¡¨
# ==============================================================================
print("\nğŸ”„ å¼€å§‹t-SNEéçº¿æ€§é™ç»´åˆ†æ...")

# t-SNEåˆ†æ - å»æ‰¹æ¬¡å‰
print("    æ­£åœ¨è¿è¡Œt-SNEé™ç»´: Before ComBat - t-SNE by Batch")
fig_tsne1, ax_tsne1 = plt.subplots(figsize=(10, 8))
plot_tsne(expression_data, batch_info, 'Before ComBat - t-SNE by Batch', ax_tsne1)
plt.tight_layout()
plt.savefig('batch_effect_before_tsne.pdf', format='pdf', dpi=300, bbox_inches='tight', facecolor='white')
plt.close()

# t-SNEåˆ†æ - å»æ‰¹æ¬¡å
print("    æ­£åœ¨è¿è¡Œt-SNEé™ç»´: After ComBat - t-SNE by Batch")
fig_tsne2, ax_tsne2 = plt.subplots(figsize=(10, 8))
plot_tsne(corrected_data, batch_info, 'After ComBat - t-SNE by Batch', ax_tsne2)
plt.tight_layout()
plt.savefig('batch_effect_after_tsne.pdf', format='pdf', dpi=300, bbox_inches='tight', facecolor='white')
plt.close()

# ä¿å­˜åˆå¹¶å›¾è¡¨
fig_batch_tsne, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))
plot_tsne(expression_data, batch_info, 'Before ComBat - t-SNE by Batch', ax1)
plot_tsne(corrected_data, batch_info, 'After ComBat - t-SNE by Batch', ax2)

# æ·»åŠ æ€»æ ‡é¢˜
fig_batch_tsne.suptitle('Batch Effect Correction - t-SNE Analysis by Batch', 
                        fontsize=20, fontweight='bold', y=0.98)
plt.tight_layout()
plt.savefig('batch_effect_tsne_by_batch.pdf', format='pdf', dpi=300, bbox_inches='tight', facecolor='white')
plt.close()

print("æ‰¹æ¬¡æ•ˆåº”t-SNEåˆ†æå›¾å·²ä¿å­˜ä¸ºPDFæ ¼å¼")

# å¦‚æœæœ‰MYC/PVT1æ³¨é‡Šä¿¡æ¯ï¼Œä¹Ÿè¿›è¡Œç›¸åº”çš„t-SNEåˆ†æ
if myc_pvt1_info is not None and len(myc_pvt1_info) > 0:
    print("\nğŸ§¬ æŒ‰MYC/PVT1åˆ†ç»„çš„t-SNEåˆ†æ...")
    
    # åªä½¿ç”¨æœ‰æ³¨é‡Šä¿¡æ¯çš„æ ·æœ¬
    annotated_samples = myc_pvt1_info.index
    filtered_expression = expression_data[annotated_samples]
    filtered_corrected = corrected_data[annotated_samples]
    
    if filtered_expression.shape[1] > 0:  # æ£€æŸ¥æ˜¯å¦æœ‰åŒ¹é…çš„æ ·æœ¬
        # t-SNEåˆ†æ - å»æ‰¹æ¬¡å‰
        fig_tsne3, ax_tsne3 = plt.subplots(figsize=(10, 8))
        plot_tsne(filtered_expression, myc_pvt1_info, 'Before ComBat - t-SNE by MYC/PVT1', ax_tsne3)
        plt.tight_layout()
        plt.savefig('myc_pvt1_before_tsne.pdf', format='pdf', dpi=300, bbox_inches='tight', facecolor='white')
        plt.close()
        
        # t-SNEåˆ†æ - å»æ‰¹æ¬¡å
        fig_tsne4, ax_tsne4 = plt.subplots(figsize=(10, 8))
        plot_tsne(filtered_corrected, myc_pvt1_info, 'After ComBat - t-SNE by MYC/PVT1', ax_tsne4)
        plt.tight_layout()
        plt.savefig('myc_pvt1_after_tsne.pdf', format='pdf', dpi=300, bbox_inches='tight', facecolor='white')
        plt.close()
        
        # ä¿å­˜åˆå¹¶å›¾è¡¨
        fig_myc_pvt1_tsne, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))
        plot_tsne(filtered_expression, myc_pvt1_info, 'Before ComBat - t-SNE by MYC/PVT1', ax1)
        plot_tsne(filtered_corrected, myc_pvt1_info, 'After ComBat - t-SNE by MYC/PVT1', ax2)
        
        # æ·»åŠ æ€»æ ‡é¢˜
        fig_myc_pvt1_tsne.suptitle('MYC/PVT1 Expression Groups - t-SNE Analysis', 
                                  fontsize=20, fontweight='bold', y=0.98)
        plt.tight_layout()
        plt.savefig('myc_pvt1_tsne_analysis.pdf', format='pdf', dpi=300, bbox_inches='tight', facecolor='white')
        plt.close()
        
        print("MYC/PVT1 t-SNEåˆ†æå›¾å·²ä¿å­˜ä¸ºPDFæ ¼å¼")
    else:
        print("è­¦å‘Šï¼šæ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„MYC/PVT1æ ·æœ¬ï¼Œè·³è¿‡MYC/PVT1 t-SNEåˆ†æ")
else:
    print("è­¦å‘Šï¼šMYC_PVT1_annotation.txtæ–‡ä»¶ä¸å­˜åœ¨æˆ–ä¸ºç©ºï¼Œè·³è¿‡MYC/PVT1 t-SNEåˆ†æ")

# ==============================================================================
# 5. ä¿å­˜å»æ‰¹æ¬¡åçš„ç»“æœ
# ==============================================================================
print("æ­£åœ¨ä¿å­˜å»æ‰¹æ¬¡åçš„ç»“æœ...")

# ä¿å­˜å»æ‰¹æ¬¡åçš„è¡¨è¾¾çŸ©é˜µ
corrected_data.to_csv('combined_expression_combat_corrected.txt', sep='\t')
print("å»æ‰¹æ¬¡åçš„è¡¨è¾¾çŸ©é˜µå·²ä¿å­˜åˆ°: combined_expression_combat_corrected.txt")

# ä¿å­˜æ‰¹æ¬¡ä¿¡æ¯
batch_info.to_csv('batch_info.txt', sep='\t', header=True)
print("æ‰¹æ¬¡ä¿¡æ¯å·²ä¿å­˜åˆ°: batch_info.txt")

# ä¿å­˜åŸå§‹åˆå¹¶çŸ©é˜µï¼ˆç”¨äºå¯¹æ¯”ï¼‰
expression_data.to_csv('combined_expression_before_combat.txt', sep='\t') 
print("åŸå§‹åˆå¹¶çŸ©é˜µå·²ä¿å­˜åˆ°: combined_expression_before_combat.txt")

# å¦‚æœæœ‰MYC_PVT1æ³¨é‡Šä¿¡æ¯ï¼Œä¹Ÿä¿å­˜å¯¹åº”çš„å­é›†
if myc_pvt1_info is not None:
    # ä¿å­˜å»æ‰¹æ¬¡åçš„MYC_PVT1æ ·æœ¬å­é›†
    annotated_samples = myc_pvt1_info.index
    filtered_corrected = corrected_data[annotated_samples]
    filtered_corrected.to_csv('myc_pvt1_expression_combat_corrected.txt', sep='\t')
    print("MYC_PVT1æ ·æœ¬å»æ‰¹æ¬¡åçŸ©é˜µå·²ä¿å­˜åˆ°: myc_pvt1_expression_combat_corrected.txt")
    
    # ä¿å­˜å»æ‰¹æ¬¡å‰çš„MYC_PVT1æ ·æœ¬å­é›†
    filtered_expression = expression_data[annotated_samples]
    filtered_expression.to_csv('myc_pvt1_expression_before_combat.txt', sep='\t')
    print("MYC_PVT1æ ·æœ¬å»æ‰¹æ¬¡å‰çŸ©é˜µå·²ä¿å­˜åˆ°: myc_pvt1_expression_before_combat.txt")

print("\næ‰€æœ‰ç»“æœå·²ä¿å­˜å®Œæˆï¼")
print("=" * 50)
print("\nè¾“å‡ºæ–‡ä»¶è¯´æ˜:")
print("  ğŸ“Š æ•°æ®æ–‡ä»¶:")
print("    1. combined_expression_combat_corrected.txt - å»æ‰¹æ¬¡åçš„è¡¨è¾¾çŸ©é˜µ (æ¨èç”¨äºåç»­åˆ†æ)")
print("    2. combined_expression_before_combat.txt - åŸå§‹åˆå¹¶çŸ©é˜µ (ç”¨äºå¯¹æ¯”)")
print("    3. batch_info.txt - æ‰¹æ¬¡ä¿¡æ¯æ–‡ä»¶")
print("    4. cohort_clustering_results.txt - Cohortèšç±»åˆ†æç»“æœ")
if myc_pvt1_info is not None:
    print("    5. myc_pvt1_expression_combat_corrected.txt - MYC_PVT1æ ·æœ¬å»æ‰¹æ¬¡åçŸ©é˜µ")
    print("    6. myc_pvt1_expression_before_combat.txt - MYC_PVT1æ ·æœ¬å»æ‰¹æ¬¡å‰çŸ©é˜µ")
print("\n  ğŸ“ˆ å›¾è¡¨æ–‡ä»¶:")
print("    1. cohort_clustering_evaluation.pdf - Cohortèšç±»è¯„ä¼°å›¾ (å®šé‡è¯„ä¼°æ ¡æ­£æ•ˆæœ)")
print("    2. batch_effect_before_tsne.pdf - æ‰¹æ¬¡æ•ˆåº”æ ¡æ­£å‰t-SNEå›¾")
print("    3. batch_effect_after_tsne.pdf - æ‰¹æ¬¡æ•ˆåº”æ ¡æ­£åt-SNEå›¾")
print("    4. batch_effect_tsne_by_batch.pdf - æ‰¹æ¬¡æ•ˆåº”t-SNEå¯¹æ¯”å›¾")
if myc_pvt1_info is not None:
    print("    5. myc_pvt1_before_tsne.pdf - MYC/PVT1æ ¡æ­£å‰t-SNEå›¾")
    print("    6. myc_pvt1_after_tsne.pdf - MYC/PVT1æ ¡æ­£åt-SNEå›¾")
    print("    7. myc_pvt1_tsne_analysis.pdf - MYC/PVT1 t-SNEå¯¹æ¯”å›¾")
print("\n  ğŸ¨ å›¾è¡¨ç‰¹è‰²:")
print("    â€¢ ä½¿ç”¨NatureæœŸåˆŠé…è‰²æ–¹æ¡ˆ")
print("    â€¢ é«˜åˆ†è¾¨ç‡300DPIè¾“å‡º")
print("    â€¢ æ‰¹æ¬¡åˆ†ç»„æ˜¾ç¤ºç½®ä¿¡æ¤­åœ†")
print("    â€¢ æ¸…æ™°çš„å»æ‰¹æ¬¡å‰åå¯¹æ¯”")
print("    â€¢ t-SNEéçº¿æ€§é™ç»´ï¼Œå¼ºè°ƒå±€éƒ¨èšç±»ç»“æ„")